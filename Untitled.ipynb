{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3722492c-5126-43a9-b728-33766e58b202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54484f0-dae8-4754-b108-844eae7afe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pymongo\n",
    "# import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb41b966-cf10-4047-9b42-07d3333ccc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# specify the URL of the news website to scrape\n",
    "url = \"https://www.theverge.com/\"\n",
    "\n",
    "# send a GET request to the website URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find all the news article links on the website\n",
    "article_links = soup.find_all(\"a\", class_=\"group-hover:shadow-underline-franklin\")\n",
    "\n",
    "l = []\n",
    "links = []\n",
    "# loop through each article link and scrape the article text\n",
    "for link in article_links:\n",
    "    # extract the URL of the article\n",
    "    article_url = link[\"href\"]\n",
    "    \n",
    "    # send a GET request to the article URL and store the response\n",
    "    article_response = requests.get(url + article_url)\n",
    "    links.append(url + article_url)\n",
    "    \n",
    "#     # parse the HTML content of the article using BeautifulSoup\n",
    "    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "    \n",
    "#     # find the main text content of the article and print it\n",
    "    article_text = article_soup.find(\"div\", class_=\"clearfix\").get_text()\n",
    "    l.append(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14cb34db-8c2a-431d-a8b5-22bd770b89fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "percentages = []\n",
    "\n",
    "nltk.data.path.append('./nltk_data/') # set the path to the NLTK data directory\n",
    "# nltk.download('punkt', download_dir='./nltk_data/') # download the 'punkt' resource to the NLTK data directory\n",
    "\n",
    "for n, i in enumerate(l):\n",
    "    # Split the paragraph into sentences\n",
    "    sentences = nltk.sent_tokenize(i)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    y = [1 if i < len(sentences)//2 else 0 for i in range(len(sentences))]  # label the first half of sentences as important\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    important_indices = clf.predict(X).nonzero()[0]  # get the indices of the important sentences\n",
    "    summary = ' '.join([sentences[i] for i in sorted(important_indices)])  # concatenate the important sentences into the summary\n",
    "    summary_percentage = round(len(summary) / len(l[n]) *100, 1)\n",
    "\n",
    "    # print(f\"{summary} {len(summary)} / {len(l[n])} \\n\")\n",
    "    summaries.append(summary)\n",
    "    percentages.append(summary_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47a0989e-89cb-4e97-af22-9efb4bd6bad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[41.1, 47.3, 53.7, 43.1, 59.8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19794657-3851-457c-b149-789ccf3ed66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for i in l:\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28ec71-c154-4d67-9d78-61c4ef5daa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "687885fc-de65-4679-8a60-dc02ca8d0894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31a073dbf41457eb948b94f89691d37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tf_model.h5\";:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a773368f564779b25d4da111fefda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1219 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"embed_positions\" \"                 f\"(type TFBartLearnedPositionalEmbedding).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1054] = 1056 is not in [0, 1026) [Op:ResourceGather]\n\nCall arguments received by layer \"embed_positions\" \"                 f\"(type TFBartLearnedPositionalEmbedding):\n  • input_shape=['tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)']\n  • past_key_values_length=0\n  • position_ids=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# generate summary\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# decode summary text\u001b[39;00m\n\u001b[0;32m     20\u001b[0m summary \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(summary_ids\u001b[38;5;241m.\u001b[39msqueeze(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\tf_utils.py:689\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[1;34m(self, input_ids, generation_config, seed, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;66;03m# 6. Prepare model inputs which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n\u001b[0;32m    688\u001b[0m     \u001b[38;5;66;03m# if encoder-decoder, we create encoder_outputs and add to `model_kwargs`\u001b[39;00m\n\u001b[1;32m--> 689\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    690\u001b[0m     \u001b[38;5;66;03m# if encoder-decoder then `input_ids` come from `decoder_start_token_id`\u001b[39;00m\n\u001b[0;32m    691\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_decoder_input_ids_for_generation(\n\u001b[0;32m    692\u001b[0m         batch_size,\n\u001b[0;32m    693\u001b[0m         decoder_start_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mdecoder_start_token_id,\n\u001b[0;32m    694\u001b[0m         bos_token_id\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mbos_token_id,\n\u001b[0;32m    695\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m    696\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation\\tf_utils.py:933\u001b[0m, in \u001b[0;36mTFGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs)\u001b[0m\n\u001b[0;32m    931\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    932\u001b[0m encoder_kwargs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmain_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 933\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    934\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m encoder_outputs\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_tf_utils.py:433\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    432\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[1;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py:778\u001b[0m, in \u001b[0;36mTFBartEncoder.call\u001b[1;34m(self, input_ids, inputs_embeds, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[0;32m    768\u001b[0m         tf\u001b[38;5;241m.\u001b[39mdebugging\u001b[38;5;241m.\u001b[39massert_less(\n\u001b[0;32m    769\u001b[0m             input_ids,\n\u001b[0;32m    770\u001b[0m             tf\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens\u001b[38;5;241m.\u001b[39minput_dim, dtype\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdtype),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m             ),\n\u001b[0;32m    775\u001b[0m         )\n\u001b[0;32m    776\u001b[0m         inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(input_ids) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_scale\n\u001b[1;32m--> 778\u001b[0m embed_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m embed_pos\n\u001b[0;32m    780\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_embedding(hidden_states)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_tf_bart.py:143\u001b[0m, in \u001b[0;36mTFBartLearnedPositionalEmbedding.call\u001b[1;34m(self, input_shape, past_key_values_length, position_ids)\u001b[0m\n\u001b[0;32m    140\u001b[0m     position_ids \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m past_key_values_length\n\u001b[0;32m    142\u001b[0m offset_dtype \u001b[38;5;241m=\u001b[39m position_ids\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(position_ids, tf\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mint32\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"embed_positions\" \"                 f\"(type TFBartLearnedPositionalEmbedding).\n\n{{function_node __wrapped__ResourceGather_device_/job:localhost/replica:0/task:0/device:CPU:0}} indices[1054] = 1056 is not in [0, 1026) [Op:ResourceGather]\n\nCall arguments received by layer \"embed_positions\" \"                 f\"(type TFBartLearnedPositionalEmbedding):\n  • input_shape=['tf.Tensor(shape=(), dtype=int32)', 'tf.Tensor(shape=(), dtype=int32)']\n  • past_key_values_length=0\n  • position_ids=None"
     ]
    }
   ],
   "source": [
    "# from transformers import BartTokenizer, TFBartForConditionalGeneration\n",
    "\n",
    "# # load tokenizer and model\n",
    "# tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "# model = TFBartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# for i in l:\n",
    "#     # tokenize input text\n",
    "#     input_text = i\n",
    "#     input_ids = tokenizer.encode(input_text, return_tensors='tf')\n",
    "\n",
    "#     # generate summary\n",
    "#     summary_ids = model.generate(input_ids=input_ids, \n",
    "#                                   max_length=100,\n",
    "#                                   min_length=30,\n",
    "#                                   num_beams=4, \n",
    "#                                   early_stopping=True)\n",
    "\n",
    "#     # decode summary text\n",
    "#     summary = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "\n",
    "#     print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce1cdb3-bd82-42fa-b914-45a6d0407e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get the current working directory of the Python script\n",
    "# cwd = os.getcwd()\n",
    "\n",
    "# # establish a connection to MongoDB\n",
    "# client = pymongo.MongoClient('mongodb://localhost:27017/')\n",
    "\n",
    "# # create a new database and collection\n",
    "# db = client['mydatabase']\n",
    "# collection = db['mycollection']\n",
    "\n",
    "# # list of text paragraphs to be stored\n",
    "# paragraphs = l\n",
    "\n",
    "# # insert each paragraph as a separate document in the collection\n",
    "# for p in paragraphs:\n",
    "#     document = {'text': p}\n",
    "#     collection.insert_one(document)\n",
    "\n",
    "# # print the documents in the collection\n",
    "# for doc in collection.find():\n",
    "#     print(doc)\n",
    "\n",
    "# # set the MongoDB data directory to the current working directory\n",
    "# db_path = os.path.join(cwd, 'mongodb_data')\n",
    "# client = pymongo.MongoClient('mongodb://localhost:27017/', serverSelectionTimeoutMS=5000, connectTimeoutMS=5000, socketTimeoutMS=5000, w=1, j=True, fsync=True, document_class=dict, tz_aware=False, connect=False, maxPoolSize=100, minPoolSize=1, maxIdleTimeMS=60000, waitQueueTimeoutMS=60000, waitQueueMultiple=5, appname=None, retryWrites=True, read_preference=pymongo.ReadPreference.PRIMARY, readConcern=pymongo.ReadConcern(level='majority'), writeConcern=pymongo.WriteConcern(w=1, j=True, fsync=True))\n",
    "# db = client['mydatabase']\n",
    "# collection = db['mycollection']\n",
    "\n",
    "# # insert new documents using the same MongoDB client and collection objects\n",
    "# new_paragraphs = [\n",
    "#     'This is a new paragraph.',\n",
    "#     'This is another new paragraph.'\n",
    "# ]\n",
    "\n",
    "# for p in new_paragraphs:\n",
    "#     document = {'text': p}\n",
    "#     collection.insert_one(document)aa\n",
    "\n",
    "# # print the updated documents\n",
    "# for doc in collection.find():\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86803e45-13c2-4930-af8f-f19dcf342aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in l:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d468f6d-e1e2-4dac-a0d7-0144884321f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# pickle.dump(nlp, open(\"save.p\", \"wb\"))\n",
    "\n",
    "\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b4eea2-d89b-4d4b-8479-3a4314869555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "print(pickle.format_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41c5f2b4-4b9a-4051-b8f1-f878d46b303c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E866] Expected a string or 'Doc' as input, but got: <class 'ellipsis'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(l):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# print(type(i), len(i))\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m i\n\u001b[1;32m---> 11\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# sentence tokenization\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:1005\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    986\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    989\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    990\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m    991\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1007\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py:1096\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_doc(doc_like)\n\u001b[1;32m-> 1096\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE866\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n",
      "\u001b[1;31mValueError\u001b[0m: [E866] Expected a string or 'Doc' as input, but got: <class 'ellipsis'>."
     ]
    }
   ],
   "source": [
    "# loading Spacy and Text input\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "summaries = []\n",
    "percentages = []\n",
    "\n",
    "for n, i in enumerate(l):\n",
    "    # print(type(i), len(i))\n",
    "    text = i\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # sentence tokenization\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    y = [1 if i < len(sentences)//2 else 0 for i in range(len(sentences))]  # label the first half of sentences as important\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X, y)\n",
    "    \n",
    "    important_indices = clf.predict(X).nonzero()[0]  # get the indices of the important sentences\n",
    "    summary = ' '.join([sentences[i] for i in sorted(important_indices)])  # concatenate the important sentences into the summary\n",
    "    summary_percentage = round(len(summary) / len(l[n]) *100, 1)\n",
    "\n",
    "    # print(f\"{summary} {len(summary)} / {len(l[n])} \\n\")\n",
    "    summaries.append(summary)\n",
    "    percentages.append(summary_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123871cc-e9e5-43f8-b4c4-7fbf1ff24005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gustav Söderström has worked at Spotify for a long time; his first big project was leading the launch of its mobile app back in 2009. That makes him the perfect company leader to talk to about Spotify’s recent redesign, which introduces a visual, TikTok-like feed for discovering new content on the app’s homepage. As his boss, CEO Daniel Ek, put it last week, it’s “the biggest change Spotify has undergone since we introduced mobile. ”With the title of co-president and chief product and technology officer, Söderström is responsible for not only how Spotify looks and feels but also all of the AI work happening behind the scenes to power its increasingly important recommendations. According to Söderström, it turns out that improving those recommendations is actually at the heart of the big redesign. “I think companies that don’t have an efficient user interface for a machine learning world are not going to be able to leverage machine learning,” he told me on the newest episode of Decoder. We spoke last week at Spotify’s Los Angeles headquarters during its Stream On event, a “State of the Union”-type presentation where new tools for podcasters, distribution deals with the likes of NPR and Patreon, and stats like Spotify now having more than 500 million monthly listeners were announced. As someone who shares Decoder’s love of org charts, Söderström talked in-depth about the reason for Spotify’s recent company restructuring that saw him elevated to the role of co-president alongside Alex Norström, who manages the business side of things. (Paid subscribers to my Command Line newsletter got an exclusive preview of our conversation last week. The next edition drops this Thursday.)Söderström and I also spoke about the pressure that comes with doing big redesigns of widely used apps like Spotify, the cost cutting and shift to focus on operational efficiency that’s happening across the tech industry, and why it’s a good thing there are no lion pits in Sweden. Hope you enjoy. The following transcript has been lightly edited for clarity. Gustav Söderström is the co-president and the chief product and technology officer of Spotify. Welcome to Decoder. Thank you so much. I’m here with you at Spotify’s headquarters in LA where you all just hosted your big annual Stream On event. You announced a bunch of things, namely the biggest redesign of Spotify in a long time. We’re going to get into all of that, but first, this is an episode of Decoder. While I’m certainly not Nilay, I do have some Decoder questions for you. Nilay and I actually share a love of org charts, believe it or not. Listen to Decoder, a show hosted by The Verge’s Nilay Patel about big ideas — and other problems. \\xa0Subscribe\\xa0here!I do too, actually. Let’s do it. I figured you did. I have a feeling that this will naturally lead into the news you all announced today. First, I’d love to start with your story. You’ve been at Spotify for a long time — 14 years. If I’m correct, I think the first big product you worked on was bringing Spotify to mobile. That’s right. The paid mobile application. Right. The company had about 30 employees at the time. Could you give me a sense of your time at Spotify and how your role has evolved over all these years?Yeah, sure. I didn’t come to Spotify as part of the founding team. I came when there were about 30 people and the desktop app had already existed for free in Sweden. Spotify was competing with piracy, and since piracy was free, we had to be free. At that time, with the government-provided internet in Sweden, that had very low latency, high bandwidth, that illusion worked on desktop. It was actually faster to play music using Spotify than it was to use Napster. \\xa0My thing at the time was mobile. Back then, it was like a special skill that was in demand. I had done a few startups and sold a few companies to American companies like Yahoo, so my thing at the time was mobile. That’s not a thing anymore — if you don’t do mobile then you basically don’t do internet. But back then, it was like a special skill that was in demand, so I was lucky to have that. Daniel [Ek, Spotify CEO] brought me on to the team so we could figure out what Spotify mobile should be. It was interesting. The first assumption, specifically from end users, was that the whole thing with Spotify was free streaming. That illusion of streaming over data didn’t work with the mobile networks we had at the time, like Edge or GPRS, once we started thinking about mobile. It didn’t work technically, and it certainly didn’t work financially. You’d get an SMS from your provider after half a song that said, “You’re out of data.” \\xa0We had to figure out what the experience was. Pretty quickly, we realized that it would have to work offline. We had to fix new licenses from labels for this offline mode that didn’t exist, that way you could encrypt and keep music locally available with a key that the user wasn’t supposed to be able to decipher and that would then expire. What was interesting for me was that there was a lot of technology innovation in making that performance seamless. When you added a file on your desktop computer, it would be silently and seamlessly synchronized so you could have it offline. It was also very much a business challenge with the labels to figure out what kind of license we needed and what that would cost. That’s what I came in to do: to head up mobile. \\xa0Over the years, I kind of label myself as a product guy, but I’m very interested in the combination of product and business models. I think there are product innovations that are interesting, and almost all the really big product innovations happen to coincide with new business models. They go hand in hand. I think that has been true for Spotify as well. So, I headed up mobile and then started heading up all of product for Spotify. A few years later, I also started heading up technology in this CPO/CTO role for Spotify. My title now is co-president, but really, my responsibilities are those of a chief product officer and a chief technology officer. Based on my understanding, it’s unusual to have the CPO also be the CTO. Have you seen that model at other companies?It’s a good question. I don’t know. Maybe it is unusual. My background is in computer science and electrical engineering. I am genuinely interested in technology for the sake of technology, but I’ve also been a CEO and founder of my own company. I have that mindset, I guess. I enjoy technology and spend a lot of time on it. I’m not a great developer, unfortunately. No one lets me touch code at Spotify. Not anymore, at least. Let’s talk about the reorg that was part of the layoffs you announced on January 23rd. Can you walk me through the reasoning for that reorg and what it is hoping to achieve? When you were named co-president along with Alex [Norström], what was going on there?Yeah, absolutely. I know you and Nilay are interested in org charts, and so am I. You follow Steven Sinofsky and Microsoft back and forth between your functional business units to see what works. The general advice is that there is no right or wrong org, but they do say you should probably stick to one. I have a slightly different version of that, which is that you can’t win with an org. There is no org that is good at everything. You could be in the worst place, where the org is bad at what’s really important to the company, yet good at the unimportant. We strive for the org to at least be good at the important things and kind of suck at the slightly less important things. That’s how we think about it. I try to be clear that you can’t win on all dimensions. It’s always a trade-off. We wanted to achieve a few things with this organization that we have now. Part of the reason for the layoffs and the org change is the macroclimate in technology. Basically, funding and money was incredibly cheap for a very long time. People are now complaining that technology companies were exuberant. The other way to say it is that when money was cheap, it made all the sense in the world to be very aggressive and grab as many opportunities as possible. We got to half a billion users and we were number one in music and podcasts, and so forth. The world has changed and that’s not true anymore. Money and capital is very expensive. Spotify has to change like everyone else. We need to become a much more efficient, focused company, in addition to being a great product. I mean, we’ve always known that the only way to build a great product for the very long term is also to be a great business. You can loan money for a while and build a great product, but you want to be self-sufficient in the long term. There is no job guarantee except being profitable. That’s the way to think about it, so that’s where we are — and by we, I mean all of technology. It’s not unexpected. This is what all new businesses look like. You have this innovation phase with startups and then it starts to mature. That’s where I think we are. \\xa0That’s part of the reason, but there is a more intrinsic reason for this. We presented this thing called the Spotify machine on investor day last summer, and we told the investor community, even though it was open to everyone, that they should think of Spotify as a music application. To build that music application, we had to build a lot of technology. We had to build a consumer application where you can find content and subscribe to it, we had to build at least two monetization modalities with an advertising tier and a subscription tier, and we had to build recommendation engines for content, and so forth. Most of our growth in podcasting is from creating new podcast users, rather than stealing existing ones from other podcast services. Then as we went into podcasts, we wanted to leverage all that infrastructure. Instead of building a separate app and starting over with zero users, we started with what was already hundreds of millions of music listeners with the bet of being able to upsell them on podcasting, which wasn’t that big at the time. Most of the growth in podcasting we’ve had is by actually creating new podcast users, rather than stealing existing ones from other podcast services. We made a bet on doubling down on our own audience and using our own distribution to our music listeners for podcasting. \\xa0Now we’re doing the same thing with audiobooks and bringing another format to our existing audience. That gives us an intrinsic distribution advantage. We already have the audience and know their tastes and habits around podcasts, which are very similar to audiobooks. It even turns out that music listening was predictive for podcast listening, which surprised me. But it comes with a challenge. If you’re going to build this thing into the same application, you’re going to make it back to trade-offs. The trade-off is that you can’t just make the application more complicated. There are benefits to that, but there are also drawbacks. From a pure designer product point of view, it’s much easier to build a separate app because you can optimize for that 100 percent. On the flip side, you’re going to start with zero users. When we looked at podcasts, I think Apple Podcasts had 98.5 percent market share. It wasn’t that there weren’t a huge amount of great podcast apps out there or that there weren’t better podcast apps than Apple’s. The problem was that no one used them. Distribution seemed to be the biggest problem, which is why we chose that trade-off. So why am I saying this? Well, I’m saying this because if we’re going to build a Spotify machine, our biggest challenge is actually to make these different content types work inside a single application, while also keeping it easy to use and not complex. \\xa0I talk about two orgs extremes, Amazon on one side and Apple on the other. This isn’t necessarily true, but if you stereotype a little bit, Amazon is known for parallel teams. You run in parallel. You divide and conquer. You have the two-pizza teams and you’re unblocked from reaching the consumer. That also results in a consumer experience where you might see three search boxes from three different teams on the screen at the same time. But it works. It’s a trillion-dollar company. It’s not like there’s nothing wrong with it, but it doesn’t optimize for simple user experience, it optimizes for speed. \\xa0I think Apple is the opposite. They ship much, much slower than Amazon, but no one gets to put their own search box there. It is centrally synchronized. They managed to build something very complex that still feels like it was built by very few people for a single user. We chose to adopt more of that. We needed to synchronize the company and eat that complexity of music, podcasts, and audiobooks — and potentially other things — for the user, instead of just shipping our org chart to the user, saying, “You figure it out. ”We built this org where we have three horizontal layers. We have a platform layer, which is the Spotify technology platform. We have the Spotify experience layer, which is all the applications, surfaces, mobile apps, cars, and desktops owned by a single person. Then we have a personalization layer. How do you choose between recommending a song, a podcast, or an audiobook for the same user at a certain moment? What is best for the user and for the company? We have these three horizontals that everything has to go through. It’s a synchronization function that actually slows things down, but then these people are forced to eat all that complexity that would otherwise end up with the end user. So these vertical businesses — the podcast business, the music business, and the audiobook business — can’t actually just go and ship stuff to the user. They have to go through these synchronization functions. That was the big org change we haven’t spoken much about externally. It is all in service of being able to do this without drastically increasing the complexity for users, to keep it simple. So far, we think it’s working. We have quantitative metrics that say consumption on Spotify is way higher than any competitor, even though they only do one thing, like music. To summarize what you just said, you’ve basically made the trade-off, “We may ship slower as an organization, but that’s okay because we’re optimizing for simplicity. That is what we’re saying. I think it’s true in those companies that it’s a trade-off you have to make. You can’t let everyone run fast in parallel and be simple at the same time. I often say that people come and ask me for more autonomy and more swim lanes. They want to own responsibility. This is a very natural thing in companies, especially American companies I find. What I say is, “No, we’re not doing competitive swimming, we’re doing synchronized swimming.” It’s a much harder sport, much harder to execute, but much more beautiful to look at when it works. Synchronized swimming that doesn’t work is just a mess. So it’s a hard thing, but it is what we’re trying to do. I’m glad you brought up the American angle actually, because I did notice Daniel’s note about the reorg — he’s the CEO of Spotify — the word “efficiency” being used. Mark Zuckerberg has used it. It’s kind of become a buzzword in the tech community this year. I think it’s kind of the theme of the year, actually. Spotify is based in Sweden, but obviously has huge roots in the American tech community. It’s a global company. Is the conversation that we’re having in the American tech world also being had in Europe and other parts where you see Spotify operating? Because interest rates globally are going up, is this a global conversation? Is this more than just America?It is a global conversation. Basically, the cost of capital has gone up globally, so that drives everything. There are local differences in Europe. As you probably know, electricity is very expensive here. You have slightly different consumer problems, so interest rates may differ slightly, but it’s largely the same conversation, I would say. Got it. This is the Decoder question. You’ve kind of already walked through this a little bit, but I’m curious how you specifically, as the head of this large organization, make decisions. Do you have a framework? How do you make decisions?So two things. One is back to the reorg and this change of Alex Norström and I becoming co-presidents. You can choose to either be more synchronized or more asynchronous. Both work, and both produce trillion-dollar companies. We chose to be more synchronized. That’s part of the reason why Daniel chose to concentrate all the functions into Alex and I.\\xa0Simplified, you can say I run product and technology and Alex runs the business part and content. Instead of dividing and conquering the problem, while we do have our own direct teams, we don’t have separate team meetings. We have a single meeting every week, where all of our direct reports meet to problem-solve, strategize, and just keep the company running. We’re literally trying to synchronize. \\xa0At Spotify, we can actually have all the decision and execution-makers in one room. That’s a competitive advantage. One of the benefits of Spotify is that it’s a big company, but it’s small compared to the big tech companies. We can actually have all the decision and execution-makers in one room, because it’s only 18 people. That’s a competitive advantage. It’s not possible at Google or Amazon anymore. They would have hundreds of people in that meeting. That is hugely important when you have an escalation or something. \\xa0What usually happens in these distributed things is that someone in a meeting says, “I’m blocked on this,” to which someone else says, “Okay, but I need to go to X. Who owns that? We’ll take it offline.” Then days go by. We’re trying to make sure that once someone says, “I’m blocked by Sarah or Yon over here,” that Sarah or Yon are in the room and can say, “Okay, what can we do about that?” That’s part of how we’re trying to be different. Then on decision-making itself, I have a big passion for a certain kind, which is sort of the Socratic debate. I love debating things. The mantra in Silicon Valley until recently has been that you move fast, break things, ship stuff, and data and code will decide arguments. I don’t subscribe that much to this. I think it has led to a bunch of stuff that wasn’t that good, and I think others are starting to feel the same way. I’ve found that you can actually reason your way through if you have the classical set of a diverse opinion group with really smart people — so the NASA idea. You can reason your way very far, even past huge problems that you would’ve run into. \\xa0We tend to reason quite a lot and use different frameworks for that. It’s the Charlie Munger quote that you should run anything through at least three frameworks. If they agree, there’s a good chance that you’re right, because any framework reduces dimensions. If you only use one, there’s a risk that your framework misses a dimension. \\xa0We do debate a lot, and that goes back to the whole speed thing. Debating and talking can feel very slow, but I often tell people that talk is cheap. So exactly for that reason, we should do a lot of it. It’s much cheaper than writing code and it’s much cheaper than shipping the wrong thing. This belief that A / B tests are cheap is completely wrong. It’s incredibly expensive to A / B test something. Which we’ll get into, because you’ve been A / B testing this big redesign that I want to talk about. Last question on orgs stuff, if you and Alex have a disagreement about something super critical that affects the whole company, is Daniel the tiebreaker?He is, technically. I mean, the benefit of this setup is that we’re not co-CEOs, we’re co-presidents. Daniel is the CEO. There is a tiebreaker, and the company is still founder-led, and I think that is critical. It is very hard for companies to be able to do big, hard things without a founder who can say, “Nope, we’re going to go left here.” It is hard. \\xa0He’s still the CEO, and that’s a big competitive advantage for this company. Most companies aren’t CEO-led or founder-led anymore in Silicon Valley. So yes, there is a tie-breaker in case that happens, but Alex and I have worked together for 10-plus years or something. So far that hasn’t happened. The idea is we debate a lot, and that can be heated. Let’s get into the big news of today and what it means for Spotify as a product. I would say the biggest thing that people are going to notice and probably have strong reactions to is this new feed you showed off today at the event you all hosted. Take me into the decision-making process for what Daniel called, “The biggest change to Spotify since the introduction of mobile.” You’re basically incorporating this TikTok-like endless visual feed to replace the main tab of the app. Why do that?I think about it this way. When we ask users what they love about Spotify, they say things like personalization and so forth, but if you drill down, what they really say is, “The more new things you show me and the more new things I find, the more I’m going to like this product.” It seems like the love for Spotify is very correlated, and hopefully causated to, the amount of discoveries we drive. You subscribe value and love to the thing that helps you discover new things and have a better life. Discovery is kind of the lifeblood of Spotify. If you take that lens, then you should expect us to try to do everything to improve discovery. The truth is, evolution has happened the last few years with these feeds that auto-play content. It has converged on the most effective way to evaluate a new piece of content. Not surprisingly, if you’re supposed to evaluate a piece of music by looking at cover art that may or may not be descriptive of the music, click through and start reading titles of songs that often have nothing to do with the music, you don’t even know the genre yet, and then click one of those titles and wait for one and a half minutes to get to the hook — that can’t be the best way to discover music. The best way to discover audio must be through audio. I don’t think it’s an accident. The world didn’t just randomize into these auto-playing cards. It is evolution. It is the most effective way to quickly understand and evaluate lots of content. We have to respect those innovations. Spotify had a different type of feed, a two-dimensional feed, which was sort of state-of-the-art many years ago, but things change. They have to, and so does Spotify. \\xa0If you think about it from the point of “the algorithm” — as they call it, even though it’s not an algorithm, it’s a ton of different systems, but to anthropomorphize a little bit — you need a user interface that can very effectively see what a user likes. Imagine that you have our old homepage, where we would show cover art for a song, you would scroll past that, and the algorithm can’t even know if you saw it. Maybe you saw it; maybe you didn’t. Maybe you saw it, thought about it, evaluated it, and didn’t like it. It can’t know that, so it’s going to repeatedly show it again and again. \\xa0If you take one of these cards, when you see this, not only does the algorithm know you saw it, it even knows you heard it. It can know that if you listened to it for a while but then continued, that you should never see it again. One way to think about it is like giving the algorithm glasses. The secret of why some of these products are so good at recommendations is not actually that they have better algorithms. It’s the same algorithms with a more efficient user interface. Yes, and more signal for the company. Yeah. So that’s the reason we’re doing it. \\xa0We’re also doing something very different, which I saw that you tweeted about today. I mentioned that while the mechanisms are the same as many other companies’, because so far right now, this is the best way that we know of to quickly evaluate lots of content. Why shouldn’t musicians get that, and podcasts, and audiobooks? Why just videos? \\xa0“We’re not optimizing for time spent in the feed, but for how much you listen to or save for later. ”We’re doing something very different, where we’re not optimizing for time spent in the feed, but for how much you listen to or save for later. This may sound a little bit like a cop-out, but it’s actually true. You look at the incentives. These other companies, the ads make money by people being in the feed. We actually don’t. First of all, we’re mostly a subscription business, so we make money from retention, and you get retention when people listen to these long things. Even in the free tier, we actually don’t make money in the feed. We make money when someone later listens to that song and there is an ad in between that and the next song. Our incentives are not to keep you in the feed. Our incentives are to let you evaluate lots of content and put it in your library, so that when you get to that background moment, you have tons of things to listen to. That’s what we want to achieve. We would like you to quite quickly get through this feed because we don’t have a lot of foreground time in Spotify. We’re a background application. \\xa0When you open it and feel like you want to find new music, we want that to be incredibly easy. What I want to achieve is that after one of these sessions you feel like, “Oh my God, my library is full of stuff that I want to listen to now, while I’m driving or when I’m running, in the background .” It is a different optimization metric. Back to the tweet, isn’t it antithetical with an endless feed? The feed isn’t actually endless. Oh, it’s not?No. So you can get to the bottom of it?You actually can. Okay, that’s interesting. I’m glad you brought up incentives and monetization, because, yes, you don’t monetize the feed currently, but I have to guess that it’s coming. I mean, aren’t video ad dollars and the margins you get from ads against podcasts a huge factor in doing something like this? Eventually there will be interstitial or unskippable ads. This is the natural evolution of this format that we’ve seen across every other category. Is that not coming?So first of all, if you’re a premium user, there aren’t ads in the music. There won’t be any in the feed for a premium user?No, not for music at all. You’re paying to not have ads there. \\xa0The thing we’re optimized for there is long-term retention. Just to stay on that point for one more second, we actually published some papers on this around machine learning. The traditional way that we recommend a song to you is impression to stream. That’s right. All right.',\n",
       " 'Apple’s launching a new way to buy an iPhone. Instead of picking one up at the store or placing an order on Apple’s website, now you can get the best of both worlds by shopping for a device online with the help of an employee over a one-way video call. The feature, called “shop with a specialist over video,” lets you chat with an Apple specialist on desktop or mobile who can guide you through the process of buying an iPhone, as well as provide more information about different iPhone features, trade-in offers, financing, and iOS. While you’ll see the person on the other end of the call in a thumbnail that appears on your screen, they won’t see you. That means you can even make the call from a device that doesn’t have a camera, although you’ll still need a microphone and speakers to chat with them. Screenshot by Emma Roth / The VergeRelatedApple reveals a yellow iPhone 14 and 14 Plus\\xa0This feature is a branch off of Apple’s existing “shop with a specialist” program, which lets you make a reservation at one of its stores to purchase products in person with the assistance of an employee. Unlike the standard shop with a specialist program, though, you can only buy iPhones.',\n",
       " 'Google has announced a suite of upcoming generative AI features for its various Workspace apps, including Google Docs, Gmail, Sheets, and Slides. The features include new ways to generate, summarize, and brainstorm text with AI in Google Docs (similar to how many people use OpenAI’s ChatGPT), the option to generate full emails in Gmail based on users’ brief bullet points, and the ability to produce\\xa0AI imagery, audio, and video to illustrate presentations in Slides (similar to features in both Microsoft Designer, powered by OpenAI’s DALL-E, and Canva, powered by Stable Diffusion). The announcement shows Google’s eagerness to catch up to competitors in the new AI race. Ever since the arrival of ChatGPT last year and Microsoft’s launch of its chatbot-enabled Bing this February, the search giant has been scrambling to launch similar AI features. The company reportedly declared a “code red” in December, with senior management telling staff to add AI tools to all its user products, which are used by billions of people, in a matter of months. But Google is definitely racing ahead of itself. Although the company has announced a raft of new features, only the first of these —\\xa0AI writing tools in Docs and Gmail —\\xa0will be made available to a group of US-based “trusted testers” this month. (This is also how Google announced availability for ChatGPT rival Bard.) Google says these and other features will then be made available to the public later in the year but didn’t specify when. You can see below the full list of AI-powered features Google says will be coming to Workspace apps in the future:Draft, reply, summarize, and prioritize your GmailBrainstorm, proofread, write, and rewrite in DocsBring your creative vision to life with auto-generated images, audio, and video in SlidesGo from raw data to insights and analysis via auto-completion, formula generation, and contextual categorization in SheetsGenerate new backgrounds and capture notes in Meet Enable workflows for getting things done in ChatAn example of AI in Google Docs turning a prompt into a full job description.  Image: GoogleOf all the new features, the AI writing and brainstorming tools in Docs and Gmail seem the most potentially useful. In a sample demo (GIF above), a user is shown the prompt “Help me write” and then enters a request: “Job post for a regional sales rep.” The AI system then completes the job spec for them in seconds, letting them edit and refine the text. Google expands on these potential functions in its press release: “Whether you’re a busy HR professional who needs to create customized job descriptions, or a parent drafting the invitation for your child’s pirate-themed birthday party, Workspace saves you the time and effort of writing that first version. Simply type a topic you’d like to write about, and a draft will instantly be generated for you. With your collaborative Al partner you can continue to refine and edit, getting more suggestions as needed. ”A similar feature will let users rewrite text or expand it using AI tools. So, says Google, you might jot down a few bullet points about a work meeting.',\n",
       " 'Meta will lay off an additional 10,000 employees through multiple rounds of cuts over the coming two months, close hiring for 5,000 open roles, and cancel more low-priority projects, CEO Mark Zuckerberg announced on Tuesday. These cuts come just four months after he laid off 11,000 employees, or 13 percent of the company, last November. The first wave of layoffs will start this week and impact Meta’s recruiting organization, followed by a second wave for tech roles in April and a third focused on business roles in late May. “My hope is to make these org changes as soon as possible in the year so we can get past this period of uncertainty and focus on the critical work ahead,” Zuckerberg wrote in a memo to employees that was also posted on his Facebook page. He called the collapse in Meta’s revenue growth last year a “humbling wake-up call” and told employees that “we should prepare ourselves for the possibility that this new economic reality will continue for many years. ”“Higher interest rates lead to the economy running leaner, more geopolitical instability leads to more volatility, and increased regulation leads to slower growth and increased costs of innovation,” he wrote. “Given this outlook, we’ll need to operate more efficiently than our previous headcount reduction to ensure success.” In a separate filing with the SEC, Meta said the new cuts will lower the high end of its expense guidance for the year by $3 billion. The company’s headcount ballooned during the pandemic from 48,000 on March 31, 2020 to 87,000 before layoffs started in November. “We should prepare ourselves for the possibility that this new economic reality will continue for many years. ”During Meta’s last earnings call in February, Zuckerberg declared this the “year of efficiency,” even as he continues to spend billions to build out his vision of the metaverse. Since November’s layoffs, he has been focused on reducing layers of management, telling employees in January, “I don’t think you want a management structure that’s just managers managing managers, managing managers, managing managers, managing the people who are doing the work.”In his memo announcing more layoffs, Zuckerberg said that it no longer makes sense for managers to have only “a few” direct reports and confirmed that many of them will be asked to become individual contributors throughout the company. “A leaner org will execute its highest priorities faster,” he wrote.',\n",
       " 'Until recently, Keychron was best known for its line of (relatively) affordable wireless mechanical keyboards with nice quality-of-life features like Mac compatibility. Then, in 2021, came the Keychron Q1, the first of over a dozen Q-series keyboards with a weighty aluminum construction, a customizable layout and switches, and great typing feel. They’re among the best off-the-shelf keyboards you can get for the money. This year’s Keychron Q1 Pro feels like a marriage of these two lines. It has the same great construction, customizability, and typing feel as the Q1 but with Bluetooth connectivity that’s every bit as reliable and easy-to-use as Keychron’s more affordable keyboards. Keychron already made good premium keyboards and good wireless keyboards\\xa0— now you can get both in the same device. At $199 (or $179 with no keycaps or switches), the Q1 Pro is still relatively pricey. But considering that’s just $20 more than a similarly specced wired Q1, with no real downsides, I think it’s the obvious choice even for people who plan to use it as a wired keyboard most of the time. Keychron was initially taking preorders for the Q1 Pro via its Kickstarter, but it’s now available to preorder directly from Keychron, with shipping expected in April.9Verge ScoreKeychron Q1 Pro$199The GoodGreat typing feelCustomizabilityLengthy battery lifeThe BadBattery life tanks with RGBNo 2.4GHz dongle option$199.00 at KeychronHow we rate and review productsYou’d be forgiven for mistaking the Q1 Pro for the original Q1 at first glance. Both keyboards use a compact laptop-style 75 percent layout, with a programmable volume dial on the top right (on the Q1, this dial was optional, but here it’s standard). Like the Q1, the Q1 Pro weighs in at around four pounds, which means it’s far too heavy to be the kind of wireless keyboard that can be easily chucked in a backpack and used while on the go. Both have hot-swap switches, are fully customizable with VIA, and have gasket-mounted plates. \\xa0Look closer, however, and the differences become more apparent. Around the top of the keyboard, you’ll find that the Q1’s physical Mac / Windows layout toggle switch has been joined by a second for hopping between Wired and Bluetooth modes or to turn the keyboard off entirely, as well as a small plastic-covered cutout in the aluminum frame to improve wireless reception. \\xa0Wireless connectivity is the big new feature for the Q1 Pro, and honestly, I struggle to fault it. Throughout my month of using the keyboard over Bluetooth with a Macbook Air, I didn’t experience any connectivity issues at all. As a test, I also tried walking around my apartment typing on the keyboard, and its connection held up just fine from other rooms or even downstairs. The keyboard can save connections to up to three devices, which is ideal if you want to quickly use it to quickly type a response to a message on your phone before swapping back to your computer. It had no problem swapping between my laptop and phone in my tests. Keycaps and switches are removable with a simple tool. The volume knob is programmable, just like the rest of the keyboard’s keys. The only complaint I have about wireless performance is that Bluetooth is your only option. That’s in contrast to other similar keyboards like the Epomaker TH80, which includes a small 2.4GHz USB dongle to use as an alternative to Bluetooth. Dongles like these are useful if your main PC doesn’t have a Bluetooth receiver, and companies like Corsair, Razer, and Logitech use them to offer a higher polling rate than what’s available over Bluetooth. But, with the Keychron Q1 Pro, Bluetooth is all you get. That means you’re stuck with a glacial 90Hz polling rate when using the keyboard wirelessly, which isn’t great for fast-paced games. Outside of games, however, I didn’t feel any lag while using the keyboard, and you can always plug it in via USB to get a more traditional 1000Hz polling rate. \\xa0Battery life is excellent when using the keyboard wirelessly — so long as you’re prepared to live without RGB lighting. With the RGB lighting at its default setting, I got four work days out of the Q1 Pro’s 4,000mAh battery and one extra day after the lighting automatically turned off to save power. So you’re effectively looking at a week of use when using RGB. But turn the lighting off entirely, and the keyboard keeps going for over a month. I last charged this keyboard six weeks ago, and it still claims to have 20 percent battery life remaining. Keychron says the keyboard offers 300 hours of battery life with the backlighting turned off, which translates to around seven and a half weeks of use, assuming you use the keyboard eight hours a day for five days a week. Double-shot keycaps mean these legends won’t fade. Ever.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c0a3a8-458a-4656-9df6-8c69956b1093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
