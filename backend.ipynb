{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cda4cbf-c7b7-48f4-8da1-8006eef4ba44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import streamlit as st\n",
    "from transformers import pipeline\n",
    "import re\n",
    "# import pymongo\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "# specify the URL of the news website to scrape\n",
    "url = \"https://www.theverge.com/\"\n",
    "\n",
    "# send a GET request to the website URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find all the news article links on the website\n",
    "article_links = soup.find_all(\"a\", class_=\"group-hover:shadow-underline-franklin\")\n",
    "timeStamp = soup.find_all(\"span\", class_=\"text-gray-63 dark:text-gray-94\")\n",
    "\n",
    "l = []\n",
    "links = []\n",
    "\n",
    "# loop through each article link and scrape the article text\n",
    "for link in article_links:\n",
    "    # extract the URL of the article\n",
    "    article_url = link[\"href\"]\n",
    "    \n",
    "    # send a GET request to the article URL and store the response\n",
    "    article_response = requests.get(url + article_url)\n",
    "    links.append(url + article_url)\n",
    "\n",
    "#     # parse the HTML content of the article using BeautifulSoup\n",
    "    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "    \n",
    "#     # find the main text content of the article and print it\n",
    "    article_text = article_soup.find(\"div\", class_=\"clearfix\").get_text()\n",
    "    l.append(article_text)\n",
    "# print(timeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88a36d90-1614-4efb-8f6e-415ff0d58495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "timeStamps = soup.find_all(\"div\", class_=\"inline-block text-gray-63 dark:text-gray-94\")\n",
    "time_stamps = [element.text for element in timeStamps]\n",
    "time_stamps = time_stamps[1:6]\n",
    "\n",
    "\n",
    "div_tags = soup.find_all('div', class_='relative block w-full aspect-square')\n",
    "img_tags = []\n",
    "\n",
    "for div in div_tags:\n",
    "    img_tags.extend(div.find_all('img'))\n",
    "\n",
    "# Extract the 'src' attribute from each image tag and filter out data URIs\n",
    "img_urls = [img['src'] for img in img_tags if 'src' in img.attrs and img['src'].startswith(('http', 'https'))]\n",
    "# print(img_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "265cbcf1-9fd2-4c59-aa90-3f343e563966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "import torch\n",
    "\n",
    "# Instantiate tokenizer and model\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\", model_max_length=512)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "summaries = []\n",
    "\n",
    "# Process each text individually\n",
    "for n, i in enumerate(l):\n",
    "    # Encode the text and create tensors\n",
    "    inputs = tokenizer.encode(\"summarize: \" + i, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    outputs = model.generate(inputs, max_length=200, min_length=80, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "    # Decode the summary and clean it\n",
    "    summary = tokenizer.decode(outputs[0])\n",
    "    clean_summary = summary.replace('<pad> ', '').replace('</s>', '')\n",
    "    summaries.append(clean_summary)\n",
    "\n",
    "    # Delete unnecessary tensors to free up memory\n",
    "    del inputs\n",
    "    del outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd1aa177-dcaa-4c9d-8759-f290ba26d00a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://duet-cdn.vox-cdn.com/thumbor/0x0:1768x1186/2400x2400/filters:focal(1221x628:1222x629):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24611115/tiktok_ai_avatars_matt_navarra.jpg', 'https://duet-cdn.vox-cdn.com/thumbor/0x0:8256x5504/2400x2400/filters:focal(4481x2527:4482x2528):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24607995/1448234892.jpg', 'https://duet-cdn.vox-cdn.com/thumbor/0x0:5805x3870/2400x2400/filters:focal(2903x1935:2904x1936):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24610430/SquidGame_Unit_101_520.jpg', 'https://duet-cdn.vox-cdn.com/thumbor/0x0:2040x1360/2400x2400/filters:focal(1020x680:1021x681):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24604962/236618_Mothers_Day_Gift_Guide_2023_WJoel.jpg', 'https://duet-cdn.vox-cdn.com/thumbor/234x70:1964x1181/2400x2400/filters:focal(1092x714:1093x715):format(webp)/cdn.vox-cdn.com/uploads/chorus_asset/file/24601774/236629_ROG_Ally_MChin_0004.jpg']\n"
     ]
    }
   ],
   "source": [
    "div_tags = soup.find_all('div', class_='relative block w-full aspect-square')\n",
    "img_tags = []\n",
    "\n",
    "for div in div_tags:\n",
    "    img_tags.extend(div.find_all('img'))\n",
    "\n",
    "# Extract the 'src' attribute from each image tag and filter out data URIs\n",
    "img_urls = [img['src'] for img in img_tags if 'src' in img.attrs and img['src'].startswith(('http', 'https'))]\n",
    "print(img_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32562ce7-34ea-4ec1-8054-07baacedeb9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://podcastaddict.com/cache/artwork/thumb/4250350', 'https://www.pbs.org/wgbh/americanexperience/media/__sized__/canonical_images/film/Vietnam_2800x1576-resize-300x0-50.jpg']\n"
     ]
    }
   ],
   "source": [
    "summaries = [\"the track, called “Not a Game,” first emerged on SoundCloud and YouTube late last week. it arrived just days after a song from someone named Ghostwriter went viral for featuring Drake’s AI-generated voice. the track is composed of multiple preexisting elements, with the primary new addition being the Drake voice. the fakers took the vocals from this a cappella rap posted to Looperman.\", \"the tool asks you to submit three to ten photos and then pick from two to five different styles for the eventual AI-made photos. the tool asks you to submit three to ten photos and then pick from two to five different styles for the eventual AI-made photos, Navarra says. Navarra shared images and video of the tool with The Verge and on twitter pretty good.\"]\n",
    "\n",
    "google_api_key = \"AIzaSyA-VlYNeVWdGE-2YK54qOmpYamLCylMdSA\"\n",
    "cx_key = \"a35fdb274e5f74f9f\"\n",
    "\n",
    "def fetch_image_url(summary):\n",
    "    query = '+'.join(re.findall(r'\\w+', summary.lower())) + \"+technology\"\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={query}&num=1&searchType=image&cx={cx_key}&imgSize=medium&key={google_api_key}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    if data and \"items\" in data and len(data[\"items\"]) > 0:\n",
    "        return data[\"items\"][0][\"link\"]\n",
    "    return None\n",
    "images = [fetch_image_url(summary) for summary in summaries]\n",
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7188464-712f-4944-98f0-41535e6801ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, image in enumerate(img_urls):\n",
    "    print(f'Image {i + 1}: {image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0da2211-8cf9-41f9-aed7-e66436cb25a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_first_thousand_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "    return ' '.join(words[:800])\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summaries = []\n",
    "for i in l:\n",
    "    i = get_first_thousand_words(i)\n",
    "\n",
    "    summaries.append(summarizer(i, max_length=250, min_length=100))\n",
    "\n",
    "summariesText = [i[0][\"summary_text\"] for i in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2721155-b523-4fd3-a386-10f7a3866b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     article_soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(article_response\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m#     # find the main text content of the article and print it\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     article_text \u001b[38;5;241m=\u001b[39m \u001b[43marticle_soup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclearfix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m()\n\u001b[0;32m     40\u001b[0m     l\u001b[38;5;241m.\u001b[39mappend(article_text)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_first_thousand_words\u001b[39m(text):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import streamlit as st\n",
    "from transformers import pipeline\n",
    "import re\n",
    "# import pymongo\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "# specify the URL of the news website to scrape\n",
    "url = \"https://www.theverge.com/\"\n",
    "\n",
    "# send a GET request to the website URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find all the news article links on the website\n",
    "article_links = soup.find_all(\"a\", class_=\"group-hover:shadow-underline-franklin\")\n",
    "\n",
    "l = []\n",
    "links = []\n",
    "\n",
    "# loop through each article link and scrape the article text\n",
    "for link in article_links:\n",
    "    # extract the URL of the article\n",
    "    article_url = link[\"href\"]\n",
    "    \n",
    "    # send a GET request to the article URL and store the response\n",
    "    article_response = requests.get(url + article_url)\n",
    "    links.append(url + article_url)\n",
    "\n",
    "#     # parse the HTML content of the article using BeautifulSoup\n",
    "    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "    \n",
    "#     # find the main text content of the article and print it\n",
    "    article_text = article_soup.find(\"div\", class_=\"clearfix\").get_text()\n",
    "    l.append(article_text)\n",
    "\n",
    "def get_first_thousand_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "    return ' '.join(words[:800])\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summaries = []\n",
    "for i in l:\n",
    "    i = get_first_thousand_words(i)\n",
    "\n",
    "    summaries.append(summarizer(i, max_length=250, min_length=100))\n",
    "\n",
    "summariesText = [i[0][\"summary_text\"] for i in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b1e59b-5a00-4042-ba15-cb542b52181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.theverge.com//2023/4/21/23692368/humane-ted-talk-imran-chaudhri-wearable-screenless-device-voice-commands-projected-screen',\n",
       " 'https://www.theverge.com//2023/4/21/23692425/apple-journaling-app-jurassic-wwdc-2023-sherlocking-ios-17',\n",
       " 'https://www.theverge.com//2023/4/21/23692254/whatsapp-keep-in-chat-disappearing-messages',\n",
       " 'https://www.theverge.com//2023/4/20/23689570/activitypub-protocol-standard-social-network',\n",
       " 'https://www.theverge.com//2023/4/20/23691831/twitter-blue-verified-celebrity-lebron-james-stephen-king']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7138f1-71f3-4b85-8934-8e00f721df77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Humane, the startup founded by ex - Apple employees Imran Chaudhri and Bethany Bongiorno, has given a first live demo of its new device. The device is a wearable gadget with a projected display and AI - powered features intended to act as a personal assistant. It is expected to be released to the public on April 22nd and is not expected to require a smartphone or other device to pair with it. In the presentation, ChaudHri uses the device to identify a chocolate bar and advise him whether or not to eat it based on his dietary requirements.',\n",
       " 'Apple could offer its own journaling app with the next update to iOS, according to a new report. The software, codenamed Jurassic, will attempt to tap into the apparent mental and physical benefits of logging your thoughts and activities on a regular basis. The app could be announced as early as Apple’s Worldwide Developers Conference in June as a feature for iOS 17. It could bring Apple into direct competition with third - party journaling apps like Day One, leading to familiar accusations that the iPhone maker is “ Sherlocking ” developers.',\n",
       " 'Mark Zuckerberg is announcing a new tweak to the service ’ s burn - after - reading feature. It allows the recipient to long - press a message and choose to keep it. WhatsApp describes the adjustment as a “ sender superpower ” It still keeps the sender in control of what ultimately happens to the message. If you decide to save a message you received and the sender is ok with that, then it will have a bookmark icon on it and you ’ ll be able to see them in your kept messages folder.',\n",
       " 'The hottest new thing in social isn’t vertical video , and it ’ s not AI - driven algorithmic feeds. Instead, it’s a little - known, years - old protocol called ActivityPub that could help rewire the entire social fabric of the internet. In recent months, a number of tech companies have thrown their resources into ActivityPub and what’ s now known as “ the Fediverse ’ ” Tumblr is working with ActivityPub, as are Flipboard, Medium, Mozilla , and even Meta.',\n",
       " 'Twitter has started getting rid of legacy blue checks for those who don’t pay up. Elon Musk said that he ’ s paying “ a few ” subscriptions “ personally , ” including for the accounts belonging to Stephen King and William Shatner. James has perhaps been the most famous hater of paid verification on Twitter. But if you check his profile , he still has a verified badge . Once you hover over the badge , it says that “ this account is verified because they are subscribed to Twitter Blue and verified their phone number . ”']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summariesText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa45e98-bb1c-4073-8f50-7e282cccc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('NewsSummarizer.db')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Delete all existing rows from the 'verge' table\n",
    "sql_command = \"DELETE FROM verge\"\n",
    "cursor.execute(sql_command)\n",
    "\n",
    "# SQL command to insert sample data into the 'verge' table\n",
    "sql_command = \"INSERT INTO verge (summary, link) VALUES (?, ?)\"\n",
    "\n",
    "# Execute the SQL command for each row of sample data\n",
    "for i in range(len(summariesText)):\n",
    "    row = (str(summariesText[i])[2:-3], links[i],)\n",
    "    cursor.execute(sql_command, row)\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e944193-a327-4e28-81da-92917195766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summariesText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83eb87a5-b3c3-43c6-87bd-15b9fbf68616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables:\n",
      "verge\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('NewsSummarizer.db')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL command to select table names\n",
    "sql_command = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "\n",
    "# Execute the SQL command\n",
    "cursor.execute(sql_command)\n",
    "\n",
    "# Fetch all the table names as a list of tuples\n",
    "table_names = cursor.fetchall()\n",
    "\n",
    "# Print the table names\n",
    "print(\"Available tables:\")\n",
    "for table in table_names:\n",
    "    print(table[0])\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1947c19-9d9b-49c2-867e-af9835c4b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = sqlite3.connect('NewsSummarizer.db')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Select all rows from the 'verge' table\n",
    "sql_command = \"SELECT * FROM verge\"\n",
    "cursor.execute(sql_command)\n",
    "\n",
    "summaries_final = []\n",
    "links_final = []\n",
    "# Fetch all rows and print them\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    summaries_final.append(row[0])\n",
    "    links_final.append(row[1])    \n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a828c29-0fef-4c4f-b9cd-0339865cf335",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d81a2-6ed5-4a36-a7ab-22f9135b198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "links = []\n",
    "for i in fromDB:\n",
    "    summaries.append(i[0])\n",
    "    links.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763ebad-fcee-4e7c-b539-12ecd939112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521e944-9aa4-45c5-848c-5f1a9ea9b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "\n",
    "# Enter your access token here\n",
    "ACCESS_TOKEN = 'sl.BbVyAS1TBPadIhcDKEl86-drmfkyVwUu54C5OoZ4TxNFEX-lzhs2HZoGI6dsaL1UXz0L2wWBrMBfYv7x-5nTeMKleoWzaKzbdBqxX1XNYkQGkwtHN0h2z8nsl-GWxn-ro7KiIkE'\n",
    "\n",
    "\n",
    "# Write the lists to a file\n",
    "with open('data.txt', 'w') as f:\n",
    "    f.write('\\n'.join(summariesText))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n'.join(links))\n",
    "\n",
    "# Create a Dropbox object\n",
    "dbx = dropbox.Dropbox(ACCESS_TOKEN)\n",
    "\n",
    "# Upload the file to Dropbox\n",
    "with open('data.txt', 'rb') as f:\n",
    "    dbx.files_upload(f.read(), '/Apps/NewsSummarizer/data.txt', mode=dropbox.files.WriteMode.overwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e26058-9b47-44f0-bd4e-fda973d313f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3067dcfd-88de-4668-97c0-9a6fb959a3a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ef84b5f-1a36-405b-9f3f-88fcbb926ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import streamlit as st\n",
    "# from transformers import pipeline\n",
    "import re\n",
    "# from streamlit.components.v1 import html\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import nltk\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "from flask import Flask, render_template\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# specify the URL of the news website to scrape\n",
    "url = \"https://www.theverge.com/\"\n",
    "\n",
    "# send a GET request to the website URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find all the news article links on the website\n",
    "article_links = soup.find_all(\"a\", class_=\"group-hover:shadow-underline-franklin\")\n",
    "\n",
    "l = []\n",
    "links = []\n",
    "\n",
    "# loop through each article link and scrape the article text\n",
    "for link in article_links:\n",
    "    # extract the URL of the article\n",
    "    article_url = link[\"href\"]\n",
    "    \n",
    "    # send a GET request to the article URL and store the response\n",
    "    article_response = requests.get(url + article_url)\n",
    "    links.append(url + article_url)\n",
    "\n",
    "#     # parse the HTML content of the article using BeautifulSoup\n",
    "    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "    \n",
    "#     # find the main text content of the article and print it\n",
    "    article_text = article_soup.find_all(\"div\", class_=\"duet--article--article-body-component\")\n",
    "    paras = []\n",
    "    concatenated_text = \"\"\n",
    "    for para in article_text:\n",
    "        concatenated_text += para.text + \" \"\n",
    "    l.append(concatenated_text.rstrip(\" \"))\n",
    "\n",
    "timeStamps = soup.find_all(\"time\", class_=\"text-gray-63 dark:text-gray-94\")\n",
    "time_stamps = [element.text for element in timeStamps]\n",
    "time_stamps = time_stamps[1:6]\n",
    "\n",
    "div_tags = soup.find_all('div', class_='relative block w-full aspect-square')\n",
    "img_tags = []\n",
    "\n",
    "for div in div_tags:\n",
    "    img_tags.extend(div.find_all('img'))\n",
    "\n",
    "# Extract the 'src' attribute from each image tag and filter out data URIs\n",
    "img_urls = [img['src'] for img in img_tags if 'src' in img.attrs and img['src'].startswith(('http', 'https'))]\n",
    "\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-small\", model_max_length=512)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "summaries = []\n",
    "\n",
    "for n, i in enumerate(l):\n",
    "    inputs = tokenizer.encode(\"summarize: \" + i, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(inputs, max_length=200, min_length=80, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0])\n",
    "    clean_summary = summary.replace('<pad> ', '').replace('</s>', '')\n",
    "    summaries.append(clean_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "792f75e1-bf7c-4d27-89b8-170dc152fd1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6:44 AM UTC', 'Jun 13', 'Jun 12', 'Jun 13', 'Jun 13']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb796a-a0ba-4eaf-857b-c470e957d77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b36f0-89a0-448a-9b54-d99a93abd5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
