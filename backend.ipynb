{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cda4cbf-c7b7-48f4-8da1-8006eef4ba44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import streamlit as st\n",
    "from transformers import pipeline\n",
    "import re\n",
    "# import pymongo\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "# specify the URL of the news website to scrape\n",
    "url = \"https://www.theverge.com/\"\n",
    "\n",
    "# send a GET request to the website URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find all the news article links on the website\n",
    "article_links = soup.find_all(\"a\", class_=\"group-hover:shadow-underline-franklin\")\n",
    "\n",
    "l = []\n",
    "links = []\n",
    "\n",
    "# loop through each article link and scrape the article text\n",
    "for link in article_links:\n",
    "    # extract the URL of the article\n",
    "    article_url = link[\"href\"]\n",
    "    \n",
    "    # send a GET request to the article URL and store the response\n",
    "    article_response = requests.get(url + article_url)\n",
    "    links.append(url + article_url)\n",
    "\n",
    "#     # parse the HTML content of the article using BeautifulSoup\n",
    "    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "    \n",
    "#     # find the main text content of the article and print it\n",
    "    article_text = article_soup.find(\"div\", class_=\"clearfix\").get_text()\n",
    "    l.append(article_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0da2211-8cf9-41f9-aed7-e66436cb25a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_first_thousand_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "    return ' '.join(words[:800])\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summaries = []\n",
    "for i in l:\n",
    "    i = get_first_thousand_words(i)\n",
    "\n",
    "    summaries.append(summarizer(i, max_length=250, min_length=100))\n",
    "\n",
    "summariesText = [i[0][\"summary_text\"] for i in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2721155-b523-4fd3-a386-10f7a3866b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import streamlit as st\n",
    "from transformers import pipeline\n",
    "import re\n",
    "# import pymongo\n",
    "# from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "# specify the URL of the news website to scrape\n",
    "url = \"https://www.theverge.com/\"\n",
    "\n",
    "# send a GET request to the website URL and store the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# parse the HTML content of the response using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# find all the news article links on the website\n",
    "article_links = soup.find_all(\"a\", class_=\"group-hover:shadow-underline-franklin\")\n",
    "\n",
    "l = []\n",
    "links = []\n",
    "\n",
    "# loop through each article link and scrape the article text\n",
    "for link in article_links:\n",
    "    # extract the URL of the article\n",
    "    article_url = link[\"href\"]\n",
    "    \n",
    "    # send a GET request to the article URL and store the response\n",
    "    article_response = requests.get(url + article_url)\n",
    "    links.append(url + article_url)\n",
    "\n",
    "#     # parse the HTML content of the article using BeautifulSoup\n",
    "    article_soup = BeautifulSoup(article_response.content, \"html.parser\")\n",
    "    \n",
    "#     # find the main text content of the article and print it\n",
    "    article_text = article_soup.find(\"div\", class_=\"clearfix\").get_text()\n",
    "    l.append(article_text)\n",
    "\n",
    "def get_first_thousand_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text)\n",
    "    return ' '.join(words[:800])\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summaries = []\n",
    "for i in l:\n",
    "    i = get_first_thousand_words(i)\n",
    "\n",
    "    summaries.append(summarizer(i, max_length=250, min_length=100))\n",
    "\n",
    "summariesText = [i[0][\"summary_text\"] for i in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b1e59b-5a00-4042-ba15-cb542b52181e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.theverge.com//2023/4/21/23692368/humane-ted-talk-imran-chaudhri-wearable-screenless-device-voice-commands-projected-screen',\n",
       " 'https://www.theverge.com//2023/4/21/23692425/apple-journaling-app-jurassic-wwdc-2023-sherlocking-ios-17',\n",
       " 'https://www.theverge.com//2023/4/21/23692254/whatsapp-keep-in-chat-disappearing-messages',\n",
       " 'https://www.theverge.com//2023/4/20/23689570/activitypub-protocol-standard-social-network',\n",
       " 'https://www.theverge.com//2023/4/20/23691831/twitter-blue-verified-celebrity-lebron-james-stephen-king']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b7138f1-71f3-4b85-8934-8e00f721df77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Humane, the startup founded by ex - Apple employees Imran Chaudhri and Bethany Bongiorno, has given a first live demo of its new device. The device is a wearable gadget with a projected display and AI - powered features intended to act as a personal assistant. It is expected to be released to the public on April 22nd and is not expected to require a smartphone or other device to pair with it. In the presentation, ChaudHri uses the device to identify a chocolate bar and advise him whether or not to eat it based on his dietary requirements.',\n",
       " 'Apple could offer its own journaling app with the next update to iOS, according to a new report. The software, codenamed Jurassic, will attempt to tap into the apparent mental and physical benefits of logging your thoughts and activities on a regular basis. The app could be announced as early as Apple’s Worldwide Developers Conference in June as a feature for iOS 17. It could bring Apple into direct competition with third - party journaling apps like Day One, leading to familiar accusations that the iPhone maker is “ Sherlocking ” developers.',\n",
       " 'Mark Zuckerberg is announcing a new tweak to the service ’ s burn - after - reading feature. It allows the recipient to long - press a message and choose to keep it. WhatsApp describes the adjustment as a “ sender superpower ” It still keeps the sender in control of what ultimately happens to the message. If you decide to save a message you received and the sender is ok with that, then it will have a bookmark icon on it and you ’ ll be able to see them in your kept messages folder.',\n",
       " 'The hottest new thing in social isn’t vertical video , and it ’ s not AI - driven algorithmic feeds. Instead, it’s a little - known, years - old protocol called ActivityPub that could help rewire the entire social fabric of the internet. In recent months, a number of tech companies have thrown their resources into ActivityPub and what’ s now known as “ the Fediverse ’ ” Tumblr is working with ActivityPub, as are Flipboard, Medium, Mozilla , and even Meta.',\n",
       " 'Twitter has started getting rid of legacy blue checks for those who don’t pay up. Elon Musk said that he ’ s paying “ a few ” subscriptions “ personally , ” including for the accounts belonging to Stephen King and William Shatner. James has perhaps been the most famous hater of paid verification on Twitter. But if you check his profile , he still has a verified badge . Once you hover over the badge , it says that “ this account is verified because they are subscribed to Twitter Blue and verified their phone number . ”']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summariesText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa45e98-bb1c-4073-8f50-7e282cccc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('NewsSummarizer.db')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Delete all existing rows from the 'verge' table\n",
    "sql_command = \"DELETE FROM verge\"\n",
    "cursor.execute(sql_command)\n",
    "\n",
    "# SQL command to insert sample data into the 'verge' table\n",
    "sql_command = \"INSERT INTO verge (summary, link) VALUES (?, ?)\"\n",
    "\n",
    "# Execute the SQL command for each row of sample data\n",
    "for i in range(len(summariesText)):\n",
    "    row = (str(summariesText[i])[2:-3], links[i],)\n",
    "    cursor.execute(sql_command, row)\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e944193-a327-4e28-81da-92917195766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summariesText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83eb87a5-b3c3-43c6-87bd-15b9fbf68616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables:\n",
      "verge\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('NewsSummarizer.db')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# SQL command to select table names\n",
    "sql_command = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "\n",
    "# Execute the SQL command\n",
    "cursor.execute(sql_command)\n",
    "\n",
    "# Fetch all the table names as a list of tuples\n",
    "table_names = cursor.fetchall()\n",
    "\n",
    "# Print the table names\n",
    "print(\"Available tables:\")\n",
    "for table in table_names:\n",
    "    print(table[0])\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1947c19-9d9b-49c2-867e-af9835c4b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "conn = sqlite3.connect('NewsSummarizer.db')\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Select all rows from the 'verge' table\n",
    "sql_command = \"SELECT * FROM verge\"\n",
    "cursor.execute(sql_command)\n",
    "\n",
    "summaries_final = []\n",
    "links_final = []\n",
    "# Fetch all rows and print them\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    summaries_final.append(row[0])\n",
    "    links_final.append(row[1])    \n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a828c29-0fef-4c4f-b9cd-0339865cf335",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d81a2-6ed5-4a36-a7ab-22f9135b198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "links = []\n",
    "for i in fromDB:\n",
    "    summaries.append(i[0])\n",
    "    links.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763ebad-fcee-4e7c-b539-12ecd939112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521e944-9aa4-45c5-848c-5f1a9ea9b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dropbox\n",
    "\n",
    "# Enter your access token here\n",
    "ACCESS_TOKEN = 'sl.BbVyAS1TBPadIhcDKEl86-drmfkyVwUu54C5OoZ4TxNFEX-lzhs2HZoGI6dsaL1UXz0L2wWBrMBfYv7x-5nTeMKleoWzaKzbdBqxX1XNYkQGkwtHN0h2z8nsl-GWxn-ro7KiIkE'\n",
    "\n",
    "\n",
    "# Write the lists to a file\n",
    "with open('data.txt', 'w') as f:\n",
    "    f.write('\\n'.join(summariesText))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n'.join(links))\n",
    "\n",
    "# Create a Dropbox object\n",
    "dbx = dropbox.Dropbox(ACCESS_TOKEN)\n",
    "\n",
    "# Upload the file to Dropbox\n",
    "with open('data.txt', 'rb') as f:\n",
    "    dbx.files_upload(f.read(), '/Apps/NewsSummarizer/data.txt', mode=dropbox.files.WriteMode.overwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8eb985-759d-46d6-af0c-68015c00350c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
